{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3bf0613-d6ec-4cf4-87e5-062fd3bd3a82",
   "metadata": {},
   "source": [
    "### Installation\n",
    "Install the packages required for executing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f1d825-84cc-43ac-9fe2-f204d77f0962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "! pip3 install --user --no-cache-dir --upgrade \"kfp>2\" \"google-cloud-pipeline-components>2\" \\\n",
    "                                        google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc6a21-604f-4a52-b904-e3bb18a61b2f",
   "metadata": {},
   "source": [
    "## Restart the kernel\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dad0c4-c173-46b8-bf99-d6e8efc35316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2207b06-771f-4dbb-a713-90c50745c0ea",
   "metadata": {},
   "source": [
    "## Check the versions of the packages you installed. The KFP SDK version should be >2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b60838-e5a2-41cd-ae93-43925343fba5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.14.4\n",
      "google-cloud-aiplatform==1.118.0\n",
      "google_cloud_pipeline_components version: 2.21.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f0bcff2-3ffb-4e51-b852-511cb10ad0f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import typing\n",
    "from typing import Dict\n",
    "from typing import NamedTuple\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components.types import artifact_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01afffb0-449b-4669-807a-793f526277fe",
   "metadata": {},
   "source": [
    "#### Project and Pipeline Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf6aad4-f675-47aa-820b-14daa796d89f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "PROJECT_ID = \"de2025-472319\"\n",
    "# The region that this pipeline runs in\n",
    "REGION = \"us-central1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "PIPELINE_ROOT = \"gs://temp_de2025_2062061\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2457ef88-cd95-4304-b6e0-143b718c44aa",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83bc305f-2456-4c07-b89f-427b0f24eaf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\",\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def download_data(project_id: str, bucket: str, file_name: str, dataset: Output[Dataset]):\n",
    "    '''download data'''\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Downloaing the file from a google bucket \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(bucket)\n",
    "    blob = bucket.blob(file_name)\n",
    "    blob.download_to_filename(dataset.path + \".csv\")\n",
    "    logging.info('Downloaded Data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7de9ca",
   "metadata": {},
   "source": [
    "## Splitting data into train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079ffea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn==1.3.2\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_test_split(dataset: Input[Dataset], dataset_train: Output[Dataset], dataset_test: Output[Dataset]):\n",
    "    '''train_test_split'''\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO) \n",
    "    \n",
    "    # Split into train and test\n",
    "    data = pd.read_csv(dataset.path+\".csv\", index_col=None)\n",
    "    train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    train.to_csv(dataset_train.path + \".csv\" , index=False, encoding='utf-8-sig')\n",
    "    test.to_csv(dataset_test.path + \".csv\" , index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3494d2-bea7-415f-9832-fcf1f2c9fe4a",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Training-XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e194fc48-1343-4bd5-9c69-f3d1ce826321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'xgboost', 'scikit-learn==1.3.2'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_xgboost (train_set: Input[Dataset], test_set: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):\n",
    "    '''train a XGBoost model with default parameters'''\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from xgboost import XGBRegressor\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "        \n",
    "    # Load the train and test sets into dataframes\n",
    "    df_train = pd.read_csv(train_set.path+\".csv\")\n",
    "    df_test = pd.read_csv(test_set.path+\".csv\")\n",
    "\n",
    "    logging.info(df_train.columns)\n",
    "    logging.info(df_test.columns)  \n",
    "        \n",
    "    # split into input (X) and output (Y) variables\n",
    "    X_train, y_train = df_train.drop('house_value', axis=1), df_train['house_value']\n",
    "    X_test, y_test = df_test.drop('house_value', axis=1), df_test['house_value']\n",
    "\n",
    "    # Train XGBoost Model to get feature importance\n",
    "    xgb_model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    # XGBoost with top 15 features\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "    # evaluate the model\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics and store them in a dictionary\n",
    "    metrics_dict = {\n",
    "        \"mse\": mean_squared_error(y_test, y_pred),\n",
    "        \"mae\": mean_absolute_error(y_test, y_pred),\n",
    "        \"r2\": r2_score(y_test, y_pred),\n",
    "        \"rmse\": np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    }    \n",
    "    logging.info(metrics_dict)  \n",
    "    \n",
    "    # Store some metadata\n",
    "    out_model.metadata[\"file_type\"] = \".pkl\"\n",
    "    out_model.metadata[\"algorithm\"] = \"xgboost\"\n",
    "    \n",
    "    # Save the model to a pickle file\n",
    "    model_file = out_model.path + \".pkl\"\n",
    "    with open(model_file, 'wb') as f:  \n",
    "        pickle.dump(xgb_model, f)   \n",
    "    \n",
    "    # Return the metrics dictionary as an output\n",
    "    outputs = NamedTuple('outputs', metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911d312-549c-4be7-bef0-e02c9d8cf80f",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Training LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acee72f0-007a-4e9c-853a-e6cf66f2a4fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_lr (train_set: Input[Dataset], test_set: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):\n",
    "    '''train a LinearRegression with default parameters'''\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    import pickle  \n",
    "       \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Load the train and test sets into dataframes\n",
    "    df_train = pd.read_csv(train_set.path+\".csv\")\n",
    "    df_test = pd.read_csv(test_set.path+\".csv\")\n",
    "\n",
    "    logging.info(df_train.columns)\n",
    "    logging.info(df_test.columns)  \n",
    "        \n",
    "    # split into input (X) and output (Y) variables\n",
    "    X_train, y_train = df_train.drop('house_value', axis=1), df_train['house_value']\n",
    "    X_test, y_test = df_test.drop('house_value', axis=1), df_test['house_value']\n",
    "\n",
    "    # Fit the model\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "   \n",
    "    # Predict using the retrained model\n",
    "    y_pred = lr_model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics and store them in a dictionary\n",
    "    metrics_dict = {\n",
    "        \"mse\": mean_squared_error(y_test, y_pred),\n",
    "        \"mae\": mean_absolute_error(y_test, y_pred),\n",
    "        \"r2\": r2_score(y_test, y_pred),\n",
    "        \"rmse\": np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    }    \n",
    "    logging.info(metrics_dict)  \n",
    "    \n",
    "    # Store some metadata\n",
    "    out_model.metadata[\"file_type\"] = \".pkl\"\n",
    "    out_model.metadata[\"algorithm\"] = \"lr\"\n",
    "    \n",
    "    # Save the model to a pickle file\n",
    "    model_file = out_model.path + \".pkl\"\n",
    "    with open(model_file, 'wb') as f:  \n",
    "        pickle.dump(lr_model, f)   \n",
    "    \n",
    "    # Return the metrics dictionary as an output\n",
    "    outputs = NamedTuple('outputs', metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c183e7-f9dd-48d7-b37e-69646bb08203",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Algorithm Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752057b5-8389-4370-97d5-19e30ebb8dd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def compare_model(xgboost_metrics: dict, lr_metrics: dict) -> str:\n",
    "    import logging\n",
    "    import json\n",
    "    import sys\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    logging.info(xgboost_metrics)\n",
    "    logging.info(lr_metrics)\n",
    "\n",
    "    xgboost_r2 = xgboost_metrics.get('r2')\n",
    "    xgboost_rmse = xgboost_metrics.get('rmse')\n",
    "\n",
    "    lr_r2 = lr_metrics.get('r2')\n",
    "    lr_rmse = lr_metrics.get('rmse')\n",
    "\n",
    "\n",
    "    # Compare metrics and select the best model\n",
    "    if xgboost_r2 > lr_r2:\n",
    "        return \"XGBoost\"\n",
    "    elif xgboost_r2 == lr_r2:\n",
    "        # If R2 scores are equal, compare RMSE\n",
    "        if xgboost_rmse < lr_rmse:\n",
    "            return \"XGBoost\"\n",
    "        else:\n",
    "            return \"LR\"\n",
    "    else:\n",
    "        return \"LR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ed599-9cd0-4215-a17d-1a8baaa2c76c",
   "metadata": {},
   "source": [
    "### Upload Model and Metrics to Google Bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9344f481-88ca-4220-a0af-3a055c61455b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def upload_model_to_gcs(project_id: str, model_repo: str, model: Input[Model]):\n",
    "    '''upload model to gsc'''\n",
    "    from google.cloud import storage   \n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)    \n",
    "  \n",
    "    # upload the model to GCS\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(model_repo)\n",
    "    blob = bucket.blob(str(model.metadata[\"algorithm\"]) + '_model' + str(model.metadata[\"file_type\"])) \n",
    "    blob.upload_from_filename(model.path + str(model.metadata[\"file_type\"]))       \n",
    "    \n",
    "    print(\"Saved the model to GCP bucket : \" + model_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444da6d9",
   "metadata": {},
   "source": [
    "# Trigger deployment of the serving app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84318942",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"google-cloud-build\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def run_build_trigger(project_id:str, trigger_id:str):\n",
    "    import sys\n",
    "    from google.cloud.devtools import cloudbuild_v1    \n",
    "    import logging \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO) \n",
    "    \n",
    "    # Create a client\n",
    "    client = cloudbuild_v1.CloudBuildClient()\n",
    "    name = f\"projects/{project_id}/locations/us-central1/triggers/{trigger_id}\"\n",
    "    # Initialize request argument(s)\n",
    "    request = cloudbuild_v1.RunBuildTriggerRequest(        \n",
    "        project_id=project_id,\n",
    "        trigger_id=trigger_id,\n",
    "        name=name\n",
    "    )\n",
    "\n",
    "    # Make the request\n",
    "    operation = client.run_build_trigger(request=request)\n",
    "    \n",
    "    logging.info(\"Trigger the CI-CD Pipeline: \" + trigger_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166590b3-f788-4e4c-8e31-fb981da56966",
   "metadata": {},
   "source": [
    "#### Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b6ae0-234b-4883-ae95-8599689a5e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the workflow of the pipeline.\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"house_price_prediction_pipeline\",)\n",
    "def pipeline(project_id: str, data_bucket: str, file_name: str, model_repo: str, trigger_id_lr: str, trigger_id_xgb: str):\n",
    "    \n",
    "    # Download the dataset\n",
    "    di_op = download_data(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=file_name\n",
    "    )\n",
    "\n",
    "    # Split the dataset into a train (80%) and test (20%) set\n",
    "    train_test_split_op = train_test_split(dataset=di_op.outputs[\"dataset\"]).after(di_op)\n",
    "\n",
    "    # Train XGBoost model\n",
    "    training_xgboost_op = train_xgboost(\n",
    "        train_set=train_test_split_op.outputs[\"dataset_train\"],\n",
    "        test_set=train_test_split_op.outputs[\"dataset_test\"]\n",
    "    ).after(train_test_split_op)\n",
    "\n",
    "    # Train Linear Regression model\n",
    "    training_lr_op = train_lr(\n",
    "        train_set=train_test_split_op.outputs[\"dataset_train\"],\n",
    "        test_set=train_test_split_op.outputs[\"dataset_test\"]\n",
    "    ).after(train_test_split_op)\n",
    "\n",
    "    # Compare the models\n",
    "    compare_model_op = compare_model(\n",
    "        xgboost_metrics=training_xgboost_op.outputs[\"metrics\"],\n",
    "        lr_metrics=training_lr_op.outputs[\"metrics\"]\n",
    "    ).after(training_xgboost_op, training_lr_op)\n",
    "\n",
    "    # Define branching based on the best model\n",
    "    with dsl.If(compare_model_op.output == \"XGBoost\"):\n",
    "        upload_xgboost_model_op = upload_model_to_gcs(\n",
    "            project_id=project_id,\n",
    "            model_repo=model_repo,\n",
    "            model=training_xgboost_op.outputs[\"out_model\"]\n",
    "        ).after(compare_model_op)\n",
    "\n",
    "        # Trigger the CI CD pipeline for building the serving app using this new model\n",
    "        trigger_model_deployment_cicd = run_build_trigger(\n",
    "            project_id=project_id,\n",
    "            trigger_id=trigger_id_xgb\n",
    "        ).after(upload_xgboost_model_op)\n",
    "\n",
    "    with dsl.If(compare_model_op.output == \"LR\"):\n",
    "        upload_lr_model_op = upload_model_to_gcs(\n",
    "            project_id=project_id,\n",
    "            model_repo=model_repo,\n",
    "            model=training_lr_op.outputs[\"out_model\"]\n",
    "        ).after(compare_model_op)\n",
    "\n",
    "        # Trigger the CI CD pipeline for building the serving app using this new model\n",
    "        trigger_model_deployment_cicd = run_build_trigger(\n",
    "            project_id=project_id,\n",
    "            trigger_id=trigger_id_lr\n",
    "        ).after(upload_lr_model_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac278200-c580-4f40-bc8b-1817d3b13c13",
   "metadata": {},
   "source": [
    "#### Compile the pipeline into a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ee4b21-89e6-4f63-845c-b249556ea919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='house-price-prediction-pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87025e-08d7-4608-b37d-c929b6eb5a3c",
   "metadata": {},
   "source": [
    "#### Submit the pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b88e89-42cd-4e64-bc4e-8e3eddebccff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "# Before initializing, make sure to set the GOOGLE_APPLICATION_CREDENTIALS\n",
    "# environment variable to the path of your service account.\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "# Prepare the pipeline job\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"house-price-prediction-pipeline\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"house-price-prediction-pipeline.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    location=REGION,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID, # makesure to use your project id \n",
    "        'data_bucket': 'data_de2025_2062061',  # makesure to use your data bucket nam \n",
    "        'file_name':'California_Houses_Processed.csv',\n",
    "        'model_repo':'models_de2025_2062061', # makesure to use your model bucket name \n",
    "        'trigger_id_lr': '09a20369-9c53-4f5c-8a91-fe9b485abe88',\n",
    "        'trigger_id_xgb': '7146f791-b966-4303-828e-1dff2e5622d2'\n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m133",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m133"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
