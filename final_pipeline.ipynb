{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3bf0613-d6ec-4cf4-87e5-062fd3bd3a82",
   "metadata": {},
   "source": [
    "### Installation\n",
    "Install the packages required for executing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f1d825-84cc-43ac-9fe2-f204d77f0962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "! pip3 install --user --no-cache-dir --upgrade \"kfp>2\" \"google-cloud-pipeline-components>2\" \\\n",
    "                                        google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc6a21-604f-4a52-b904-e3bb18a61b2f",
   "metadata": {},
   "source": [
    "## Restart the kernel\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dad0c4-c173-46b8-bf99-d6e8efc35316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2207b06-771f-4dbb-a713-90c50745c0ea",
   "metadata": {},
   "source": [
    "## Check the versions of the packages you installed. The KFP SDK version should be >2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b60838-e5a2-41cd-ae93-43925343fba5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.14.4\n",
      "google-cloud-aiplatform==1.118.0\n",
      "google_cloud_pipeline_components version: 2.21.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f0bcff2-3ffb-4e51-b852-511cb10ad0f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import typing\n",
    "from typing import Dict\n",
    "from typing import NamedTuple\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components.types import artifact_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01afffb0-449b-4669-807a-793f526277fe",
   "metadata": {},
   "source": [
    "#### Project and Pipeline Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf6aad4-f675-47aa-820b-14daa796d89f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "PROJECT_ID = \"de2025-472319\"\n",
    "# The region that this pipeline runs in\n",
    "REGION = \"us-central1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "PIPELINE_ROOT = \"gs://temp_de2025_2062061\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2457ef88-cd95-4304-b6e0-143b718c44aa",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83bc305f-2456-4c07-b89f-427b0f24eaf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\",\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def download_data(project_id: str, bucket: str, file_name: str, dataset: Output[Dataset]):\n",
    "    '''download data'''\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Downloaing the file from a google bucket \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(bucket)\n",
    "    blob = bucket.blob(file_name)\n",
    "    blob.download_to_filename(dataset.path + \".csv\")\n",
    "    logging.info('Downloaded Data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7de9ca",
   "metadata": {},
   "source": [
    "## Splitting data into train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079ffea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn==1.3.2\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_test_split(dataset: Input[Dataset], dataset_train: Output[Dataset], dataset_test: Output[Dataset]):\n",
    "    '''train_test_split'''\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO) \n",
    "    \n",
    "    # Split into train and test\n",
    "    data = pd.read_csv(dataset.path+\".csv\", index_col=None)\n",
    "    train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    train.to_csv(dataset_train.path + \".csv\" , index=False, encoding='utf-8-sig')\n",
    "    test.to_csv(dataset_test.path + \".csv\" , index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3494d2-bea7-415f-9832-fcf1f2c9fe4a",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Training-XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e194fc48-1343-4bd5-9c69-f3d1ce826321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'xgboost', 'scikit-learn==1.3.2'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_xgboost (features: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):\n",
    "    '''train a XGBoost model with default parameters'''\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from xgboost import XGBRegressor\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "        \n",
    "    # Load the train and test sets into dataframes\n",
    "    df_train = pd.read_csv(train_set.path+\".csv\")\n",
    "    df_test = pd.read_csv(test_set.path+\".csv\")\n",
    "\n",
    "    logging.info(df_train.columns)\n",
    "    logging.info(df_test.columns)  \n",
    "        \n",
    "    # split into input (X) and output (Y) variables\n",
    "    X_train, y_train = df_train.drop('house_value', axis=1), df_train['house_value']\n",
    "    X_test, y_test = df_test.drop('house_value', axis=1), df_test['house_value']\n",
    "\n",
    "    # Train XGBoost Model to get feature importance\n",
    "    xgb_model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_importances = xgb_model.feature_importances_\n",
    "    xgb_indices = np.argsort(xgb_importances)[::-1][:15]  # Top 15 features for XGBoost\n",
    "    top_15_features_xgb = X_train.columns[xgb_indices]\n",
    "\n",
    "    # Train-Test Split with Top 15 Features\n",
    "    X_train = X_train[top_15_features_xgb]\n",
    "    X_test = X_test[top_15_features_xgb]\n",
    "\n",
    "    # Grid Search for XGBoost with Top 15 Features\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.8, 1]\n",
    "    }\n",
    "\n",
    "    # Grid Search\n",
    "    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best XGBoost model after grid search\n",
    "    best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "    # XGBoost with top 15 features\n",
    "    y_pred = best_xgb_model.predict(X_test)\n",
    "\n",
    "    # evaluate the model\n",
    "    y_pred = best_xgb_model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics and store them in a dictionary\n",
    "    metrics_dict = {\n",
    "        \"mse\": mean_squared_error(y_test, y_pred),\n",
    "        \"mae\": mean_absolute_error(y_test, y_pred),\n",
    "        \"r2\": r2_score(y_test, y_pred),\n",
    "        \"rmse\": np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    }    \n",
    "    logging.info(metrics_dict)  \n",
    "    \n",
    "    # Store some metadata\n",
    "    out_model.metadata[\"file_type\"] = \".pkl\"\n",
    "    out_model.metadata[\"algorithm\"] = \"xgboost\"\n",
    "    \n",
    "    # Save the model to a pickle file\n",
    "    model_file = out_model.path + \".pkl\"\n",
    "    with open(model_file, 'wb') as f:  \n",
    "        pickle.dump(best_xgb_model, f)   \n",
    "    \n",
    "    # Return the metrics dictionary as an output\n",
    "    outputs = NamedTuple('outputs', metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911d312-549c-4be7-bef0-e02c9d8cf80f",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Training LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acee72f0-007a-4e9c-853a-e6cf66f2a4fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_lr (features: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):\n",
    "    '''train a LinearRegression with default parameters'''\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    import pickle  \n",
    "       \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Load the train and test sets into dataframes\n",
    "    df_train = pd.read_csv(train_set.path+\".csv\")\n",
    "    df_test = pd.read_csv(test_set.path+\".csv\")\n",
    "\n",
    "    logging.info(df_train.columns)\n",
    "    logging.info(df_test.columns)  \n",
    "        \n",
    "    # split into input (X) and output (Y) variables\n",
    "    X_train, y_train = df_train.drop('house_value', axis=1), df_train['house_value']\n",
    "    X_test, y_test = df_test.drop('house_value', axis=1), df_test['house_value']\n",
    "\n",
    "    # Fit the model\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    lr_importances = np.abs(lr_model.coef_)  # Absolute value of coefficients\n",
    "    lr_indices = np.argsort(lr_importances)[::-1][:15]  # Top 15 features for Linear Regression\n",
    "    top_15_features_lr = X_train.columns[lr_indices]\n",
    "    \n",
    "    # Train-Test Split with Top 15 Features\n",
    "    X_train = X_train[top_15_features_lr]\n",
    "    X_test = X_test[top_15_features_lr]\n",
    "\n",
    "    # Retrain model using only top 15 features\n",
    "    lr_model_top_15 = LinearRegression()\n",
    "    lr_model_top_15.fit(X_train, y_train)\n",
    "\n",
    "    # Predict using the retrained model\n",
    "    y_pred = lr_model_top_15.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics and store them in a dictionary\n",
    "    metrics_dict = {\n",
    "        \"mse\": mean_squared_error(y_test, y_pred),\n",
    "        \"mae\": mean_absolute_error(y_test, y_pred),\n",
    "        \"r2\": r2_score(y_test, y_pred),\n",
    "        \"rmse\": np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    }    \n",
    "    logging.info(metrics_dict)  \n",
    "    \n",
    "    # Store some metadata\n",
    "    out_model.metadata[\"file_type\"] = \".pkl\"\n",
    "    out_model.metadata[\"algorithm\"] = \"lr\"\n",
    "    \n",
    "    # Save the model to a pickle file\n",
    "    model_file = out_model.path + \".pkl\"\n",
    "    with open(model_file, 'wb') as f:  \n",
    "        pickle.dump(lr_model_top_15, f)   \n",
    "    \n",
    "    # Return the metrics dictionary as an output\n",
    "    outputs = NamedTuple('outputs', metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c183e7-f9dd-48d7-b37e-69646bb08203",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Algorithm Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752057b5-8389-4370-97d5-19e30ebb8dd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def compare_model(xgboost_metrics: dict, lr_metrics: dict) -> str:\n",
    "    import logging\n",
    "    import json\n",
    "    import sys\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    logging.info(xgboost_metrics)\n",
    "    logging.info(lr_metrics)\n",
    "    if xgboost_metrics.get(\"r2\") > lr_metrics.get(\"r2\"):\n",
    "        return \"XGBoost\"\n",
    "    else :\n",
    "        return \"LR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ed599-9cd0-4215-a17d-1a8baaa2c76c",
   "metadata": {},
   "source": [
    "### Upload Model and Metrics to Google Bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9344f481-88ca-4220-a0af-3a055c61455b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def upload_model_to_gcs(project_id: str, model_repo: str, model: Input[Model]):\n",
    "    '''upload model to gsc'''\n",
    "    from google.cloud import storage   \n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)    \n",
    "  \n",
    "    # upload the model to GCS\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(model_repo)\n",
    "    blob = bucket.blob(str(model.metadata[\"algo\"]) + '_model' + str(model.metadata[\"file_type\"])) \n",
    "    blob.upload_from_filename(model.path + str(model.metadata[\"file_type\"]))       \n",
    "    \n",
    "    print(\"Saved the model to GCP bucket : \" + model_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166590b3-f788-4e4c-8e31-fb981da56966",
   "metadata": {},
   "source": [
    "#### Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a96b6ae0-234b-4883-ae95-8599689a5e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the workflow of the pipeline.\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"diabetes-prdictor-training-pipeline\")\n",
    "def pipeline(project_id: str, data_bucket: str, trainset_filename: str, model_repo: str, testset_filename: str):\n",
    "    \n",
    "    \n",
    "    di_op = download_data(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=trainset_filename\n",
    "    )\n",
    "\n",
    " \n",
    "    training_mlp_job_run_op = train_mlp(\n",
    "        features=di_op.outputs[\"dataset\"]\n",
    "    )\n",
    "    \n",
    "     \n",
    "    training_lr_job_run_op = train_lr(\n",
    "        features=di_op.outputs[\"dataset\"]\n",
    "    )\n",
    "    \n",
    "    pre_di_op = download_data(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=testset_filename\n",
    "    ).after(training_mlp_job_run_op, training_lr_job_run_op)\n",
    "        \n",
    "        \n",
    "    comp_model__op = compare_model(mlp_metrics=training_mlp_job_run_op.outputs[\"metrics\"],\n",
    "                                       lr_metrics=training_lr_job_run_op.outputs[\"metrics\"]).after(training_mlp_job_run_op, training_lr_job_run_op)  \n",
    "    \n",
    "    # defining the branching condition\n",
    "    with dsl.If(comp_model__op.output==\"MLP\"):\n",
    "        predict_mlp_job_run_op = predict_mlp(\n",
    "            model=training_mlp_job_run_op.outputs[\"out_model\"],      \n",
    "            features=pre_di_op.outputs[\"dataset\"]\n",
    "        )\n",
    "        upload_model_mlp_to_gc_op = upload_model_to_gcs(\n",
    "            project_id=project_id,\n",
    "            model_repo=model_repo,\n",
    "            model=training_mlp_job_run_op.outputs['out_model']\n",
    "        ).after(predict_mlp_job_run_op)\n",
    "        \n",
    "    with dsl.If(comp_model__op.output==\"LR\"):\n",
    "        predict_lr_job_run_op = predict_lr(\n",
    "            model=training_lr_job_run_op.outputs[\"out_model\"],     \n",
    "            features=pre_di_op.outputs[\"dataset\"]\n",
    "        )\n",
    "        upload_model_lr_to_gc_op = upload_model_to_gcs(\n",
    "            project_id=project_id,\n",
    "            model_repo=model_repo,\n",
    "            model=training_lr_job_run_op.outputs['out_model']\n",
    "        ).after(predict_lr_job_run_op)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac278200-c580-4f40-bc8b-1817d3b13c13",
   "metadata": {},
   "source": [
    "#### Compile the pipeline into a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8ee4b21-89e6-4f63-845c-b249556ea919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='diabetes_predictor_training_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87025e-08d7-4608-b37d-c929b6eb5a3c",
   "metadata": {},
   "source": [
    "#### Submit the pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83b88e89-42cd-4e64-bc4e-8e3eddebccff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "# Before initializing, make sure to set the GOOGLE_APPLICATION_CREDENTIALS\n",
    "# environment variable to the path of your service account.\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    staging_bucket=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "# Prepare the pipeline job\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"diabetes-predictor-ct-pipeline\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"diabetes_predictor_training_pipeline.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    location=REGION,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID, # makesure to use your project id \n",
    "        'data_bucket': 'data_de2025',  # makesure to use your data bucket name \n",
    "        'trainset_filename': 'training_set.csv',     # makesure to upload these to your data bucket from DE2024/lab4/data\n",
    "        'testset_filename': 'prediction_set.csv',    # makesure to upload these to your data bucket from DE2024/lab4/data\n",
    "        'model_repo':'models_de2025' # makesure to use your model bucket name \n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m133",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m133"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
